{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
      "metadata": {
        "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # load the data from google drive and load into dataframes\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Released_Data/'\n",
        "# sub_class = 'subclass_mapping.csv'\n",
        "# super_class = 'superclass_mapping.csv'\n",
        "# train_data = 'train_data.csv'\n",
        "\n",
        "# # Now you can read or manipulate the file\n",
        "# df_sub = pd.read_csv(file_path+'subclass_mapping.csv')\n",
        "# df_subclass = pd.read_csv(file_path+sub_class)\n",
        "# df_superclass = pd.read_csv(file_path+super_class)\n",
        "# df_train = pd.read_csv(file_path+train_data)\n",
        "\n",
        "# # Example: Print the content\n",
        "# df_subclass.head()\n",
        "\n",
        "train_ann_df = pd.read_csv(file_path + 'train_data.csv')\n",
        "super_map_df = pd.read_csv(file_path + 'superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv(file_path + 'subclass_mapping.csv')\n",
        "\n",
        "train_img_dir = file_path + 'train_shuffle'\n",
        "test_img_dir = file_path + 'test_shuffle'\n",
        "\n",
        "# count_subclass_87 = len(train_ann_df[train_ann_df['subclass_index'] == 87]) by default, there are 0 novel classes in training, so we should create our own\n",
        "\n",
        "# all classes with subclass 45, 57, and 29, 78 -> should get subclass label of 87 (novel)\n",
        "\n",
        "# target_classes = [45, 57, 29, 78]\n",
        "\n",
        "# Update the subclass_index to 87 for the specified classes\n",
        "# train_ann_df.loc[train_ann_df['subclass_index'].isin(target_classes), 'subclass_index'] = 87\n",
        "\n",
        "\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=(0), std=(1)),\n",
        "# ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeG0lHc93QNC",
        "outputId": "20fce351-b9ae-4061-ac93-cbf382fff4ad"
      },
      "id": "YeG0lHc93QNC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform data manipulation: rotation, swapping, etc so that we can increase the size of the data set\n",
        "\n",
        "# Suggestion:\n",
        "# You should aim to train a generalizable model with all the techniques we have discussed\n",
        "# so far (e.g., data augmentation, weight decay, etc.) Other tricks may include potentially\n",
        "# building your local validation set for testing the model's generalization ability.\n",
        "\n",
        "# Data augmentation and normalization\n",
        "from torchvision import transforms\n",
        "\n",
        "# Training data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with scaling\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Testing data transformations\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize to 224x224\n",
        "    transforms.CenterCrop(224),  # Center crop to maintain the aspect ratio\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Apply data augmentation and normalization to datasets\n",
        "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=transform_train)\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=transform_test)\n"
      ],
      "metadata": {
        "id": "h165tAEj42bv"
      },
      "id": "h165tAEj42bv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# account for unlabeled data ()\n",
        "\n",
        "# https://cdn-uploads.piazza.com/paste/jcwopj0bkHVK/4e633d4b629ae6f0c6630bba588e0d783805da7fd03580939c93a350d54adde5/NNDL_Multi-label_Classification_Competition.pdf"
      ],
      "metadata": {
        "id": "pM9I9OE-5B4m"
      },
      "id": "pM9I9OE-5B4m",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482"
      },
      "outputs": [],
      "source": [
        "# train_ann_df = pd.read_csv('train_data.csv')\n",
        "# super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "# sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "# train_img_dir = 'train_shuffle'\n",
        "# test_img_dir = 'test_shuffle'\n",
        "\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=(0), std=(1)),\n",
        "# ])\n",
        "\n",
        "# Create train and val split\n",
        "# train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "# test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(32, 32),\n",
        "            ResidualBlock(32, 32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock(32, 64, stride=2),\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(64, 64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(64, 128, stride=2),\n",
        "            ResidualBlock(128, 128),\n",
        "            ResidualBlock(128, 128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Adjusted the input size for the first fully connected layer\n",
        "        self.fc1 = nn.Linear(2*2*128, 256)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, 4)\n",
        "        self.fc3b = nn.Linear(128, 88)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "# Instantiate the model\n",
        "resnet_model = ResNet()\n"
      ],
      "metadata": {
        "id": "e0vRJiKa6254"
      },
      "id": "e0vRJiKa6254",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
      "metadata": {
        "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840"
      },
      "outputs": [],
      "source": [
        "# Simple CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "                        nn.Conv2d(64, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.fc1 = nn.Linear(4*4*128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, 4)\n",
        "        self.fc3b = nn.Linear(128, 88)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "        return super_out, sub_out\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            print('1')\n",
        "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "            print('2')\n",
        "            self.optimizer.zero_grad()\n",
        "            print('3')\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            print('4')\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                test_predictions['image'].append(img_name[0])\n",
        "                test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
      "metadata": {
        "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1"
      },
      "outputs": [],
      "source": [
        "# Init model and trainer\n",
        "device = 'cuda'\n",
        "model = ResNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7941c289-d9b1-4714-b788-898b3b889f58",
      "metadata": {
        "id": "7941c289-d9b1-4714-b788-898b3b889f58",
        "outputId": "63942377-9bfd-4e92-881b-aea7b738d936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    torch.cuda.empty_cache()\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
      "metadata": {
        "id": "16d17e37-1a08-4ae1-8517-a16ff4769622"
      },
      "outputs": [],
      "source": [
        "trainer.test(save_to_csv=False, return_predictions=True)\n",
        "\n",
        "'''\n",
        "This simple baseline scores the following test accuracy\n",
        "\n",
        "Superclass Accuracy\n",
        "Overall: 43.83 %\n",
        "Seen: 61.11 %\n",
        "Unseen: 0.00 %\n",
        "\n",
        "Subclass Accuracy\n",
        "Overall: 2.03 %\n",
        "Seen: 9.56 %\n",
        "Unseen: 0.00 %\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399",
      "metadata": {
        "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}