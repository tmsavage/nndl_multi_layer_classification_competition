{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAQWdXUF4MqQ"
      },
      "outputs": [],
      "source": [
        "# citation: https://www.datacamp.com/tutorial/autoencoder-classifier-python\n",
        "# citation: https://drive.google.com/drive/folders/1idfa8y7esf7usGo7SSxsH4iKBECEPFNr?usp=share_linkÂ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdR6Zn8p7LmR"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnE3Xlc5puE"
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTvIx0cx5pwi"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def upscale_image(self, image):\n",
        "        # Upscale the image using bicubic interpolation\n",
        "        width, height = image.size\n",
        "        upscaled_image = image.resize((128,128), Image.BICUBIC)\n",
        "        return upscaled_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Upscale the image\n",
        "        image = self.upscale_image(image)\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWIr-uHI5py4"
      },
      "outputs": [],
      "source": [
        "train_ann_df = pd.read_csv('train_data.csv')\n",
        "super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "train_img_dir = 'train_shuffle'\n",
        "test_img_dir = 'test_shuffle'\n",
        "\n",
        "image_preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0), std=(1)),\n",
        "])\n",
        "\n",
        "# Create train and val split\n",
        "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
        "print(len(train_dataset))\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP_zlBlG5p1v"
      },
      "outputs": [],
      "source": [
        "# CNN\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, 3, padding='same'),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 16, 3, padding='same'),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, 3, padding='same'),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(2097152, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 3 * 3 * 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(128, 3, 3)),\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, output_padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ConvTranspose2d(64, 32, 3,stride=2, output_padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, output_padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ConvTranspose2d(8, 3, 3, stride=2, output_padding=0)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(48387, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, 4)\n",
        "        self.fc3b = nn.Linear(128, 88)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "        return super_out, sub_out\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cpu'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.max_super_prob_all = []\n",
        "        self.max_sub_prob_all = []\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                super_probs = F.softmax(super_outputs, dim=1)\n",
        "                sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "\n",
        "                # Get maximum probability values and corresponding predicted classes\n",
        "                max_super_prob, super_predicted = torch.max(super_probs, 1)\n",
        "                max_sub_prob, sub_predicted = torch.max(sub_probs, 1)\n",
        "\n",
        "                # max_super_prob, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                # max_sub_prob, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                print('max_super_prob:', max_super_prob)\n",
        "                print('max_sub_prob:', max_sub_prob)\n",
        "\n",
        "                self.max_super_prob_all.append(max_super_prob)\n",
        "                self.max_sub_prob_all.append(max_sub_prob)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acccc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                # Commented is the old method\n",
        "                # _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                # _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                 # Apply softmax to get probabilities\n",
        "                super_probs = F.softmax(super_outputs, dim=1)\n",
        "                sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "\n",
        "                # is_novel_super = max_prob < threshold\n",
        "\n",
        "                # Get maximum probability values and corresponding predicted classes\n",
        "                max_super_prob, super_predicted = torch.max(super_probs, 1)\n",
        "                max_sub_prob, sub_predicted = torch.max(sub_probs, 1)\n",
        "\n",
        "                super_threshold = 0.3 # 0.4 gave 0.41053 --> need to drop this to 0.05 or 0.1 and retest\n",
        "                sub_threshold = 0.4 # because the mean is around 0.6 and the mean is much smaller\n",
        "\n",
        "                sub_predicted[max_sub_prob < sub_threshold] = 87\n",
        "                super_predicted[max_super_prob < super_threshold] = 3\n",
        "\n",
        "                test_predictions['image'].append(img_name[0])\n",
        "                test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions\n",
        "\n",
        "    def max_probs_all(self):\n",
        "        return self.max_super_prob_all, self.max_sub_prob_all\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et12xXKq6Iun"
      },
      "outputs": [],
      "source": [
        "# Init model and trainer\n",
        "device = 'cuda'\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHhR15_S6Ixc"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYpBSYQy6PIQ"
      },
      "outputs": [],
      "source": [
        "trainer.test(save_to_csv=True, return_predictions=True)\n",
        "\n",
        "'''\n",
        "This simple baseline scores the following test accuracy\n",
        "\n",
        "Superclass Accuracy\n",
        "Overall: 43.83 %\n",
        "Seen: 61.11 %\n",
        "Unseen: 0.00 %\n",
        "\n",
        "Subclass Accuracy\n",
        "Overall: 2.03 %\n",
        "Seen: 9.56 %\n",
        "Unseen: 0.00 %\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr2f4QgD6k5w"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "file_path = 'example_test_predictions.csv'  # Replace with the actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# Select two columns\n",
        "selected_columns = df[['image', 'subclass_index']]\n",
        "\n",
        "# Rename one of the columns\n",
        "selected_columns = selected_columns.rename(columns={'image': 'ID'})\n",
        "selected_columns = selected_columns.rename(columns={'subclass_index': 'Target'})\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "output_file_path = 'sub_test.csv'  # Replace with the desired output file path\n",
        "selected_columns.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Select two columns\n",
        "selected_column = df[['image', 'superclass_index']]\n",
        "\n",
        "# Rename one of the columns\n",
        "selected_column = selected_column.rename(columns={'image': 'ID'})\n",
        "selected_column = selected_column.rename(columns={'superclass_index': 'Target'})\n",
        "print(selected_columns)\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "output_file_path = 'super_test.csv'  # Replace with the desired output file path\n",
        "selected_column.to_csv(output_file_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
