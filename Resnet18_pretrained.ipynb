{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
      "metadata": {
        "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"
      },
      "outputs": [],
      "source": [
        "# citation: https://drive.google.com/drive/folders/1idfa8y7esf7usGo7SSxsH4iKBECEPFNr?usp=share_linkÂ \n",
        "# citation: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09265dca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "# class MultiClassImageTestDataset(Dataset):\n",
        "#     def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "#         self.super_map_df = super_map_df\n",
        "#         self.sub_map_df = sub_map_df\n",
        "#         self.img_dir = img_dir\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self): # Count files in img_dir\n",
        "#         return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_name = str(idx) + '.jpg'\n",
        "#         img_path = os.path.join(self.img_dir, img_name)\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "#         super_idx = self.ann_df['superclass_index'][idx]\n",
        "#         super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "#         sub_idx = self.ann_df['subclass_index'][idx]\n",
        "#         sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image, super_idx, super_label, sub_idx, sub_label, img_name\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "YeG0lHc93QNC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeG0lHc93QNC",
        "outputId": "c387615a-6596-4d5d-ad3e-8b5f07358f79"
      },
      "outputs": [],
      "source": [
        "# # load the data from google drive and load into dataframes\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "file_path = 'Released_Data/'\n",
        "file_path_new_data = 'new_data/'\n",
        "# sub_class = 'subclass_mapping.csv'\n",
        "# super_class = 'superclass_mapping.csv'\n",
        "# train_data = 'train_data.csv'\n",
        "\n",
        "# # Now you can read or manipulate the file\n",
        "# df_sub = pd.read_csv(file_path+'subclass_mapping.csv')\n",
        "# df_subclass = pd.read_csv(file_path+sub_class)\n",
        "# df_superclass = pd.read_csv(file_path+super_class)\n",
        "# df_train = pd.read_csv(file_path+train_data)\n",
        "\n",
        "# # Example: Print the content\n",
        "# df_subclass.head()\n",
        "\n",
        "# train_ann_df = pd.read_csv(file_path + 'train_data.csv')\n",
        "train_ann_df = pd.read_csv('train_data.csv')\n",
        "super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "# train_img_dir = file_path + 'train_shuffle'\n",
        "train_img_dir = 'train_shuffle'\n",
        "test_img_dir = 'test_shuffle'\n",
        "\n",
        "# count_subclass_87 = len(train_ann_df[train_ann_df['subclass_index'] == 87]) by default, there are 0 novel classes in training, so we should create our own\n",
        "\n",
        "# all classes with subclass 45, 57, and 29, 78 -> should get subclass label of 87 (novel)\n",
        "\n",
        "# target_classes = [45, 57, 29, 78]\n",
        "\n",
        "# Update the subclass_index to 87 for the specified classes\n",
        "# train_ann_df.loc[train_ann_df['subclass_index'].isin(target_classes), 'subclass_index'] = 87\n",
        "\n",
        "\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=(0), std=(1)),\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "h165tAEj42bv",
      "metadata": {
        "id": "h165tAEj42bv"
      },
      "outputs": [],
      "source": [
        "# perform data manipulation: rotation, swapping, etc so that we can increase the size of the data set\n",
        "\n",
        "# Suggestion:\n",
        "# You should aim to train a generalizable model with all the techniques we have discussed\n",
        "# so far (e.g., data augmentation, weight decay, etc.) Other tricks may include potentially\n",
        "# building your local validation set for testing the model's generalization ability.\n",
        "\n",
        "# Data augmentation and normalization\n",
        "from torchvision import transforms\n",
        "\n",
        "# # Training data transformations\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with scaling\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Testing data transformations\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.Resize(224),  # Resize to 224x224\n",
        "#     transforms.CenterCrop(224),  # Center crop to maintain the aspect ratio\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "\n",
        "# Training data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(64),  # Random crop with scaling\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Testing data transformations\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(64),  # Resize to 224x224\n",
        "    transforms.CenterCrop(64),  # Center crop to maintain the aspect ratio\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Apply data augmentation and normalization to datasets\n",
        "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=transform_train)\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=transform_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "pM9I9OE-5B4m",
      "metadata": {
        "id": "pM9I9OE-5B4m"
      },
      "outputs": [],
      "source": [
        "# account for unlabeled data ()\n",
        "\n",
        "# https://cdn-uploads.piazza.com/paste/jcwopj0bkHVK/4e633d4b629ae6f0c6630bba588e0d783805da7fd03580939c93a350d54adde5/NNDL_Multi-label_Classification_Competition.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482"
      },
      "outputs": [],
      "source": [
        "# train_ann_df = pd.read_csv('train_data.csv')\n",
        "# super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "# sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "# train_img_dir = 'train_shuffle'\n",
        "# test_img_dir = 'test_shuffle'\n",
        "\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=(0), std=(1)),\n",
        "# ])\n",
        "\n",
        "# Create train and val split\n",
        "# train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "# test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e0vRJiKa6254",
      "metadata": {
        "id": "e0vRJiKa6254"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fbafbe",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
      "metadata": {
        "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840"
      },
      "outputs": [],
      "source": [
        "# \n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cpu'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            # print('1')\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "            # print('2')\n",
        "            self.optimizer.zero_grad()\n",
        "            # print('3')\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            # print('4')\n",
        "            # prob dist = [0.2, 0.05, ...]\n",
        "\n",
        "            # threasholding, \n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                test_predictions['image'].append(img_name[0])\n",
        "                test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions\n",
        "\n",
        "\n",
        "\n",
        "# \n",
        "class SubclassTrainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cpu'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            # print('1')\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "            # print('2')\n",
        "            self.optimizer.zero_grad()\n",
        "            # print('3')\n",
        "            # super_outputs, sub_outputs = self.model(inputs)\n",
        "            sub_outputs = self.model(inputs)\n",
        "            # print('4')\n",
        "            # prob dist = [0.2, 0.05, ...]\n",
        "\n",
        "            # threasholding, \n",
        "            loss = self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "                sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(sub_outputs, sub_labels)\n",
        "                # _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                # super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        # print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=True, return_predictions=True):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                sub_outputs = self.model(inputs)\n",
        "                # _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                test_predictions['ID'].append(img_name[0])\n",
        "                # test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['Target'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c1f4e5bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pretrained model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18  # Using a smaller ResNet variant for 32x32 images\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Load the pretrained ResNet model\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Modify the last layers for your specific task\n",
        "num_classes = 88  # Adjust based on your task\n",
        "in_features = model.fc.in_features\n",
        "\n",
        "# Replace the fully connected layer with your custom layers\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(in_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n",
        "\n",
        "\n",
        "#CASE 1\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_superclasses, num_subclasses):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.resnet = resnet18(pretrained=True)\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_superclasses + num_subclasses)\n",
        "        )\n",
        "        self.num_superclasses = num_superclasses\n",
        "        self.num_subclasses = num_subclasses\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        super_outputs, sub_outputs = x[:, :self.num_superclasses], x[:, self.num_superclasses:]\n",
        "        return super_outputs, sub_outputs\n",
        "\n",
        "num_superclasses = 4\n",
        "num_subclasses = 88\n",
        "\n",
        "model = CustomResNet(num_superclasses, num_subclasses)\n",
        "\n",
        "\n",
        "# CASE 2\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Define the model\n",
        "# class CustomResNet(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(CustomResNet, self).__init__()\n",
        "#         self.resnet = resnet50(pretrained=True)\n",
        "#         in_features = self.resnet.fc.in_features\n",
        "#         self.resnet.fc = nn.Identity()  # Remove the last fully connected layer\n",
        "\n",
        "#         # Add custom layers\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc1 = nn.Linear(in_features, 512)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "#         self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.resnet(x)\n",
        "#         x = self.flatten(x)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# Instantiate the model\n",
        "# model = CustomResNet(num_classes=88)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
      "metadata": {
        "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1"
      },
      "outputs": [],
      "source": [
        "# Init model and trainer\n",
        "device = 'cpu'\n",
        "model = ResNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "7941c289-d9b1-4714-b788-898b3b889f58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7941c289-d9b1-4714-b788-898b3b889f58",
        "outputId": "0b83c0c6-4e14-4e5c-b2ce-99313060c283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mvalidate_epoch()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[22], line 80\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# print('3')\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m super_outputs, sub_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print('4')\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# prob dist = [0.2, 0.05, ...]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# threasholding, \u001b[39;00m\n\u001b[1;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(super_outputs, super_labels) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(sub_outputs, sub_labels)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    # torch.cuda.empty_cache()\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
      "metadata": {
        "id": "16d17e37-1a08-4ae1-8517-a16ff4769622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           image  superclass_index  subclass_index\n",
            "0          0.jpg                 0              27\n",
            "1          1.jpg                 0              72\n",
            "2          2.jpg                 0              14\n",
            "3          3.jpg                 0              27\n",
            "4          4.jpg                 2              57\n",
            "...          ...               ...             ...\n",
            "12372  12372.jpg                 1              64\n",
            "12373  12373.jpg                 0              77\n",
            "12374  12374.jpg                 1              80\n",
            "12375  12375.jpg                 1              77\n",
            "12376  12376.jpg                 1              65\n",
            "\n",
            "[12377 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# predictions = trainer.test(save_to_csv=True, return_predictions=True)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "prediction_subclass = predictions\n",
        "\n",
        "# Rename columns\n",
        "# prediction_subclass.columns = [\"ID\", \"Target\"] + list(df.columns[2:])\n",
        "\n",
        "\n",
        "\n",
        "# '''\n",
        "# This simple baseline scores the following test accuracy\n",
        "\n",
        "# Superclass Accuracy\n",
        "# Overall: 43.83 % # 30%\n",
        "# Seen: 61.11 %\n",
        "# Unseen: 0.00 %\n",
        "\n",
        "# Subclass Accuracy\n",
        "# Overall: 2.03 % # above 7 target\n",
        "# Seen: 9.56 %\n",
        "# Unseen: 0.00 %\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399",
      "metadata": {
        "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
